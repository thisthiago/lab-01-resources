{"paragraphs":[{"text":"%md\n# O que é um RDD?\n\nRDD (Resilient Distributed Dataset) é a estrutura de dados fundamental do Spark. Ele representa uma coleção imutável (que não pode ser alterada) de objetos que podem ser processados em paralelo em um cluster.\n\nTransformações vs Ações:\n\n - Transformações (map, filter, etc.) são lazy (não executam imediatamente)\n\n - Ações (collect, count, etc.) forçam a execução\n\n\n1. Persistência: Você pode armazenar RDDs em memória com rdd.persist() para reutilização\n2. Particionamento: RDDs são divididos em partições para processamento paralelo\n3. Imutabilidade: RDDs não podem ser alterados, apenas transformados em novos RDDs","user":"anonymous","dateUpdated":"2025-04-14T19:31:24+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>O que é um RDD?</h1>\n<p>RDD (Resilient Distributed Dataset) é a estrutura de dados fundamental do Spark. Ele representa uma coleção imutável (que não pode ser alterada) de objetos que podem ser processados em paralelo em um cluster.</p>\n<p>Transformações vs Ações:</p>\n<ul>\n<li>\n<p>Transformações (map, filter, etc.) são lazy (não executam imediatamente)</p>\n</li>\n<li>\n<p>Ações (collect, count, etc.) forçam a execução</p>\n</li>\n</ul>\n<ol>\n<li>Persistência: Você pode armazenar RDDs em memória com rdd.persist() para reutilização</li>\n<li>Particionamento: RDDs são divididos em partições para processamento paralelo</li>\n<li>Imutabilidade: RDDs não podem ser alterados, apenas transformados em novos RDDs</li>\n</ol>\n\n</div>"}]},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1744658824813_1654258978","id":"paragraph_1744658824813_1654258978","dateCreated":"2025-04-14T19:27:04+0000","dateStarted":"2025-04-14T19:31:24+0000","dateFinished":"2025-04-14T19:31:24+0000","status":"FINISHED","focus":true},{"title":"Configuração inicial","text":"%pyspark\r\n# O contexto do Spark (sc) já está disponível no Zeppelin\r\nprint(\"SparkContext disponível:\", sc)","user":"anonymous","dateUpdated":"2025-04-14T19:25:19+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"title":true,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"SparkContext disponível: <SparkContext master=local[*] appName=spark-shared_process>\n"}]},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1744658022106_1271521007","id":"paragraph_1744658022106_1271521007","dateCreated":"2025-04-14T19:13:42+0000","dateStarted":"2025-04-14T19:25:19+0000","dateFinished":"2025-04-14T19:25:20+0000","status":"FINISHED"},{"title":"Criando um RDD básico","text":"%pyspark\ndados = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(dados)\n\n# Coletar e mostrar os dados\nprint(rdd.collect())  # [1, 2, 3, 4, 5]","user":"anonymous","dateUpdated":"2025-04-14T19:25:30+0000","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[1, 2, 3, 4, 5]\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://9382a0befda4:4040/jobs/job?id=10"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1744658036933_1173308578","id":"paragraph_1744658036933_1173308578","dateCreated":"2025-04-14T19:13:56+0000","dateStarted":"2025-04-14T19:25:30+0000","dateFinished":"2025-04-14T19:25:30+0000","status":"FINISHED"},{"title":"Transformações básicas (map, filter)","text":"%pyspark\n# Dobrar todos os valores (map)\nrdd_dobro = rdd.map(lambda x: x * 2)\nprint(rdd_dobro.collect())  # [2, 4, 6, 8, 10]\n\n# Filtrar apenas números pares (filter)\nrdd_pares = rdd.filter(lambda x: x % 2 == 0)\nprint(rdd_pares.collect())  # [2, 4]","user":"anonymous","dateUpdated":"2025-04-14T19:25:46+0000","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[2, 4, 6, 8, 10]\n[2, 4]\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://9382a0befda4:4040/jobs/job?id=11"},{"jobUrl":"http://9382a0befda4:4040/jobs/job?id=12"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1744658270587_1314026570","id":"paragraph_1744658270587_1314026570","dateCreated":"2025-04-14T19:17:50+0000","dateStarted":"2025-04-14T19:25:46+0000","dateFinished":"2025-04-14T19:25:47+0000","status":"FINISHED"},{"title":"Ações comuns","text":"%pyspark\nrdd = sc.parallelize([1, 2, 3, 4, 5])\n\n# Contar elementos\nprint(\"Count:\", rdd.count())\n\n# Soma\nprint(\"Sum:\", rdd.sum())\n\n# Primeiro elemento\nprint(\"First:\", rdd.first())\n\n# Pegar os 3 primeiros\nprint(\"Take 3:\", rdd.take(3))","user":"anonymous","dateUpdated":"2025-04-14T19:25:54+0000","progress":0,"config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Count: 5\nSum: 15\nFirst: 1\nTake 3: [1, 2, 3]\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://9382a0befda4:4040/jobs/job?id=13"},{"jobUrl":"http://9382a0befda4:4040/jobs/job?id=14"},{"jobUrl":"http://9382a0befda4:4040/jobs/job?id=15"},{"jobUrl":"http://9382a0befda4:4040/jobs/job?id=16"},{"jobUrl":"http://9382a0befda4:4040/jobs/job?id=17"},{"jobUrl":"http://9382a0befda4:4040/jobs/job?id=18"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1744658460324_223551136","id":"paragraph_1744658460324_223551136","dateCreated":"2025-04-14T19:21:00+0000","dateStarted":"2025-04-14T19:25:54+0000","dateFinished":"2025-04-14T19:25:57+0000","status":"FINISHED"},{"title":"MapReduce","text":"%pyspark\ntexto = [\"olá mundo\", \"olá spark\", \"mundo big data\"]\nrdd_texto = sc.parallelize(texto)\n\n# Passo 1: Dividir as frases em palavras (flatMap)\npalavras = rdd_texto.flatMap(lambda frase: frase.split(\" \"))\n\n# Passo 2: Mapear cada palavra para um par (palavra, 1)\npares = palavras.map(lambda palavra: (palavra, 1))\n\n# Passo 3: Reduzir somando as contagens\ncontagem = pares.reduceByKey(lambda a, b: a + b)\n\n# Resultado final\nprint(contagem.collect())\n# Resultado: [('olá', 2), ('mundo', 2), ('spark', 1), ('big', 1), ('data', 1)]\n","user":"anonymous","dateUpdated":"2025-04-14T19:26:52+0000","progress":0,"config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[('spark', 1), ('olá', 2), ('mundo', 2), ('data', 1), ('big', 1)]\n"}]},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://9382a0befda4:4040/jobs/job?id=19"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1744658337712_2046337688","id":"paragraph_1744658337712_2046337688","dateCreated":"2025-04-14T19:18:57+0000","dateStarted":"2025-04-14T19:25:58+0000","dateFinished":"2025-04-14T19:26:01+0000","status":"FINISHED"}],"name":"Spark RDD","id":"2KRPYR8WR","defaultInterpreterGroup":"spark","version":"0.12.0","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{},"path":"/Spark RDD"}